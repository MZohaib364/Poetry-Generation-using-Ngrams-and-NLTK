{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd097a40",
      "metadata": {
        "id": "fd097a40"
      },
      "source": [
        "# Question1\n",
        " POETRY Generation using N-grams\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbfd8420",
      "metadata": {
        "id": "bbfd8420"
      },
      "source": [
        "1 Introduction:\n",
        "In this assignment, you will use n-gram language modeling to generate some poetry using the ngrams. For the purpose of this assignment a poem will consist of three stanzas each containing four verses where each verse consists of 7—10 words. For example, following is a manually generated stanza.\n",
        "\n",
        "دل سے نکال یاس کہ زندہ ہوں میں ابھی،\n",
        "\n",
        "ہوتا ہے کیوں اداس کہ زندہ ہوں میں ابھی،\n",
        "\n",
        "مایوسیوں کی قید سے خود کو نکال کر،\n",
        "\n",
        "آ جاؤ میرے پاس کہ زندہ ہوں میں ابھی،\n",
        "\n",
        "\n",
        "\n",
        "آ کر کبھی تو دید سے سیراب کر مجھے،\n",
        "\n",
        "مرتی نہیں ہے پیاس کہ زندہ ہوں میں ابھی،\n",
        "\n",
        "مہر و وفا خلوص و محبت گداز دل،\n",
        "\n",
        "سب کچھ ہے میرے پاس کہ زندہ ہوں میں ابھی،\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "لوٹیں گے تیرے آتے ہی پھر دن بہار کے،\n",
        "\n",
        "رہتی ہے دل میں آس کہ زندہ ہوں میں،\n",
        "\n",
        "نایا ب شاخ چشم میں کھلتے ہیں اب بھی خواب، سچ ہے ترا\n",
        "\n",
        "قیاس کہ زندہ ہوں میں ابھی\n",
        "\n",
        "The task is to print three such stanzas with an empty line in between. The generation model can be trained on the provided Poetry Corpus containing poems from Faiz, Ghalib and Iqbal.You can scrape other urdu poetry too from internet. You will train unigram and bigram models using this corpus. These models will be used to generate poetry.\n",
        "\n",
        "2 Assignment Task:\n",
        "\n",
        "The task is to generate a poem using different models. We will generate a poem verse by verse until all stanzas have been generated. The poetry generation problem can be solved using the following algorithm:\n",
        "1. Load the Poetry Corpus\n",
        "2. Tokenize the corpus in order to split it into a list of words\n",
        "3. Generate n-gram models\n",
        "4. For each of the stanzas\n",
        "– For each verse\n",
        "* Generate a random number in the range [7...10]\n",
        "* Select first word\n",
        "* Select subsequent words until end of verse\n",
        "* [bonus] If not the first verse, try to rhyme the last word with the last word of the previous verse\n",
        "* Print verse\n",
        "– Print empty line after stanza\n",
        "2.1 Implementation Challenges:\n",
        "\n",
        "Among the challenges of solving this assignment will be selecting subsequent words once we have chosen the first word of the verse. To predict the next word, what we aim to compute is the most probable next word from all the possible next words. In other words, we need to find the set of words that occur most frequently after the already selected word and choose the next word from that set. We can use a Conditional Frequency Distribution (CFD) to figure that out! A CFD tells us: given a condition, what is likelihood of each possible outcome. [bonus] Rhyming the generated verses is also a challenge. You can build your dictionary for rhyming. The Urdu sentence is written from right to left, so makes your n-gram models according to this style.\n",
        "\n",
        "2.2 Standard n-gram Models\n",
        "We can develop our model using the Conditional Frequency Distribution method. First develop a unigram model (Unigram Model), then the bigram model (Bigram Model) and then trigram model. Select the first word of each line randomly from starting words in the vocabulary and then use the bigram model to generate the next word until the verse is complete. Generate the next three lines similarly.\n",
        " Follow the same steps for the trigram model and compare the results of the two n-gram models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce978bb3-395d-4c3b-bb12-63cd7243fcf8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "ce978bb3-395d-4c3b-bb12-63cd7243fcf8",
        "outputId": "be464961-e430-4874-d4e6-a038a58e687f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2081a3e3de60>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the Poetry Corpus and tokenize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcorpus_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/muhammadzohaib/Semester_3/PAI/A4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwordlists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.*\\.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, fileids, word_tokenizer, sent_tokenizer, para_block_reader, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcorpus\u001b[0m \u001b[0minto\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mCorpusReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, fileids, encoding, tagset)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPathPointer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CorpusReader: expected a string or a PathPointer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file or directory: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/home/muhammadzohaib/Semester_3/PAI/A4'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk import ngrams\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "# Load the Poetry Corpus and tokenize it\n",
        "corpus_root = '/home/muhammadzohaib/Semester_3/PAI/A4'\n",
        "wordlists = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
        "words = wordlists.words()\n",
        "\n",
        "# Define the number of stanzas and verses\n",
        "num_stanzas = 5\n",
        "num_verses_per_stanza = 4\n",
        "\n",
        "# Define the maximum verse length\n",
        "max_verse_length = 10\n",
        "verse_length = random.randint(7, max_verse_length)\n",
        "\n",
        "# Unigram Model\n",
        "unigram_model = nltk.FreqDist(words)\n",
        "\n",
        "# Bigram Model\n",
        "bigrams = list(ngrams(words, 2))\n",
        "cfd_bigram = nltk.ConditionalFreqDist(bigrams)\n",
        "\n",
        "# Trigram Model\n",
        "trigrams = list(ngrams(words, 3))\n",
        "cfd_trigram = nltk.ConditionalFreqDist(((w1, w2), w3) for w1, w2, w3 in trigrams)\n",
        "\n",
        "# Generate poems using unigram, bigram, and trigram models\n",
        "for _ in range(num_stanzas):\n",
        "    for _ in range(num_verses_per_stanza):\n",
        "\n",
        "\n",
        "        # Select the first word randomly from the vocabulary\n",
        "        first_word = random.choice(words)\n",
        "\n",
        "        # Initialize the verse with the first word\n",
        "        verse_unigram = [first_word]\n",
        "        verse_bigram = [first_word]\n",
        "        verse_trigram = [first_word]\n",
        "\n",
        "        for _ in range(verse_length - 1):\n",
        "            # Use Unigram Model to generate the next word\n",
        "            next_word_unigram = unigram_model.max()\n",
        "            verse_unigram.append(next_word_unigram)\n",
        "\n",
        "            # Use Bigram Model to generate the next word\n",
        "            context_bigram = (verse_bigram[-1],)\n",
        "            if context_bigram in cfd_bigram:\n",
        "                next_word_bigram = cfd_bigram[context_bigram].max()\n",
        "            else:\n",
        "                # Fallback to a random word if the context is not found\n",
        "                next_word_bigram = random.choice(words)\n",
        "            verse_bigram.append(next_word_bigram)\n",
        "\n",
        "             # Use Trigram Model to generate the next word\n",
        "            if len(verse_trigram) >= 2:\n",
        "                context_trigram = (verse_trigram[-2], verse_trigram[-1])\n",
        "                if context_trigram in cfd_trigram:\n",
        "                    next_word_trigram = cfd_trigram[context_trigram].max()\n",
        "                else:\n",
        "                    # Fallback to a random word if the context is not found\n",
        "                    next_word_trigram = random.choice(words)\n",
        "                verse_trigram.append(next_word_trigram)\n",
        "            else:\n",
        "                # If there are not enough words for the trigram context, use bigram context\n",
        "                context_bigram = (verse_bigram[-1],)\n",
        "                if context_bigram in cfd_bigram:\n",
        "                    next_word_trigram = cfd_bigram[context_bigram].max()\n",
        "                else:\n",
        "                    # Fallback to a random word if the context is not found\n",
        "                    next_word_trigram = random.choice(words)\n",
        "                verse_trigram.append(next_word_trigram)\n",
        "\n",
        "\n",
        "        # Print the generated verses for each model\n",
        "        # print(\"Unigram Model:\")\n",
        "        # print(' '.join(verse_unigram))\n",
        "        # print()\n",
        "\n",
        "        # print(\"Bigram Model:\")\n",
        "        # print(' '.join(verse_bigram))\n",
        "        # print()\n",
        "\n",
        "        # print(\"Trigram Model:\")\n",
        "        print(' '.join(verse_trigram))\n",
        "        print()\n",
        "\n",
        "    # Print an empty line after each stanza\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb709115",
      "metadata": {
        "id": "bb709115"
      },
      "outputs": [],
      "source": [
        "#Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2effe45e-260f-4829-862b-e65281d15583",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2effe45e-260f-4829-862b-e65281d15583",
        "outputId": "1d2407fa-f0f4-4f60-8b1c-82e43da2a936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c485c182-8b66-4b7a-9333-c76c76b490ae",
      "metadata": {
        "id": "c485c182-8b66-4b7a-9333-c76c76b490ae"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "corpus_root = '/home/muhammadzohaib/Semester_3/PAI/A4'\n",
        "wordlists = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
        "\n",
        "# file_id = 'ghalib.txt'\n",
        "# raw_text = wordlists.raw(file_id)\n",
        "\n",
        "file_ids = wordlists.fileids()  # Get a list of file IDs in the corpus\n",
        "\n",
        "for file_id in file_ids:\n",
        "    raw_text = wordlists.raw(file_id)\n",
        "    words = word_tokenize(raw_text)\n",
        "\n",
        "unigrams = nltk.ngrams(words, 1)\n",
        "unigramFD = nltk.FreqDist(unigrams)\n",
        "\n",
        "bigrams = nltk.ngrams(words, 2)\n",
        "bigramFD = nltk.FreqDist(bigrams)\n",
        "\n",
        "trigrams = nltk.ngrams(words, 3)\n",
        "trigramFD = nltk.FreqDist(trigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387e2e48-4322-4f5c-aaf4-600eab6ad65e",
      "metadata": {
        "id": "387e2e48-4322-4f5c-aaf4-600eab6ad65e",
        "outputId": "c8d71d8c-7cef-4464-ae09-6ac2bd4b2df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ConditionalFreqDist with 1870 conditions>\n"
          ]
        }
      ],
      "source": [
        "# bigrams = nltk.ngrams(words, 2)\n",
        "# cfd1 = nltk.ConditionalFreqDist(bigrams)\n",
        "\n",
        "# print(cfd1)\n",
        "\n",
        "from nltk import ngrams\n",
        "\n",
        "n = 2  # Change this to 1 for unigrams and 3 for trigrams\n",
        "n_grams = list(ngrams(words, n))\n",
        "\n",
        "# Create a conditional frequency distribution for n-grams\n",
        "cfd = nltk.ConditionalFreqDist(n_grams)\n",
        "\n",
        "# Verify the content of the CFD\n",
        "print(cfd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8454ab8a-dbbd-47c6-9e92-4a313abcae9e",
      "metadata": {
        "id": "8454ab8a-dbbd-47c6-9e92-4a313abcae9e",
        "outputId": "f9b518e6-c9d9-4f20-a214-2d233af64041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ConditionalFreqDist with 1845 conditions>\n",
            "غریب   اگرچہ   مغربیوں   کا   یہ   بات   کہ\n",
            "بن   کے   لیے   تنگ   چیتے   کا   یہ\n",
            "نومیدی   مجھے   نکتۂ   سلمان   خوش   آہنگ   دنیا\n",
            "اور   بھی   ہیں   وہ   خاک   کہ   میں\n",
            "\n",
            "میں   نے   آخر   جو   آوارۂ   جنوں   تھے\n",
            "وہی   رہے   ہیں   وہ   خاک   کہ   میں\n",
            "میں   نے   آخر   جو   آوارۂ   جنوں   تھے\n",
            "کا   یہ   بات   کہ   میں   نے   آخر\n",
            "\n",
            "لولوئے   لالا   اگر   کیفیت   ہے   یا   میرا\n",
            "جہان   مے   خانہ   ہر   کوئی   بادہ   و\n",
            "ہے   یا   میرا   یا   میرا   یا   میرا\n",
            "ازلی   ہے   یا   میرا   یا   میرا   یا\n",
            "\n",
            "آیا   ہے   یا   میرا   یا   میرا   یا\n",
            "پارس   یہ   بات   کہ   میں   نے   آخر\n",
            "اہل   خرد   کا   یہ   بات   کہ   میں\n",
            "ستارے   یہ   بات   کہ   میں   نے   آخر\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "\n",
        "corpus_root = '/home/muhammadzohaib/Semester_3/PAI/A4'\n",
        "wordlists = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
        "\n",
        "# file_id = 'ghalib.txt'\n",
        "# raw_text = wordlists.raw(file_id)\n",
        "\n",
        "file_ids = wordlists.fileids()  # Get a list of file IDs in the corpus\n",
        "pattern = r'[،؛؟!\"#\\$%&\\'\\(\\)\\*\\+,\\-\\.\\/:؛;<=>?@\\[\\\\\\]^_`{|}~]'\n",
        "\n",
        "for file_id in file_ids:\n",
        "    raw_text = wordlists.raw(file_id)\n",
        "    raw_text = re.sub(pattern, '', raw_text)\n",
        "    words = word_tokenize(raw_text)\n",
        "\n",
        "unigrams = nltk.ngrams(words, 1)\n",
        "unigramFD = nltk.FreqDist(unigrams)\n",
        "\n",
        "bigrams = nltk.ngrams(words, 2)\n",
        "bigramFD = nltk.FreqDist(bigrams)\n",
        "\n",
        "trigrams = nltk.ngrams(words, 3)\n",
        "trigramFD = nltk.FreqDist(trigrams)\n",
        "\n",
        "# bigrams = nltk.ngrams(words, 2)\n",
        "# cfd1 = nltk.ConditionalFreqDist(bigrams)\n",
        "\n",
        "# print(cfd1)\n",
        "\n",
        "from nltk import ngrams\n",
        "\n",
        "n = 2  # Change this to 1 for unigrams and 3 for trigrams\n",
        "n_grams = list(ngrams(words, n))\n",
        "\n",
        "# Create a conditional frequency distribution for n-grams\n",
        "cfd = nltk.ConditionalFreqDist(n_grams)\n",
        "\n",
        "# Verify the content of the CFD\n",
        "print(cfd)\n",
        "\n",
        "import random\n",
        "\n",
        "cfd1 = nltk.ConditionalFreqDist(bigrams)\n",
        "n=2\n",
        "\n",
        "length = random.randint(7,10)\n",
        "\n",
        "for i in range(4):\n",
        "\n",
        "    for j in range(4):\n",
        "\n",
        "        verse = [random.choice(words)]\n",
        "\n",
        "        for k in range(length-1):\n",
        "\n",
        "            context = verse[-1]\n",
        "            # print(context)\n",
        "            next_word = cfd[context].max()\n",
        "            verse.append(' ')\n",
        "            verse.append(next_word)\n",
        "\n",
        "        print(\" \".join(verse))\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9836dac6-5fc4-4c16-ab0c-c2840f11f031",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "9836dac6-5fc4-4c16-ab0c-c2840f11f031",
        "outputId": "934850ec-ecd6-4b95-ca00-6406340e7894"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ab6cea625921>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorpus_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/muhammadzohaib/Semester_3/PAI/A4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mwordlists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaintextCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.*\\.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# file_id = 'ghalib.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, fileids, word_tokenizer, sent_tokenizer, para_block_reader, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcorpus\u001b[0m \u001b[0minto\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \"\"\"\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mCorpusReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sent_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, fileids, encoding, tagset)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPathPointer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CorpusReader: expected a string or a PathPointer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file or directory: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: No such file or directory: '/home/muhammadzohaib/Semester_3/PAI/A4'"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "\n",
        "corpus_root = '/home/muhammadzohaib/Semester_3/PAI/A4'\n",
        "wordlists = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
        "\n",
        "# file_id = 'ghalib.txt'\n",
        "# raw_text = wordlists.raw(file_id)\n",
        "\n",
        "file_ids = wordlists.fileids()  # Get a list of file IDs in the corpus\n",
        "pattern = r'[،؛؟!\"#\\$%&\\'\\(\\)\\*\\+,\\-\\.\\/:؛;<=>?@\\[\\\\\\]^_-`{|}~]'\n",
        "\n",
        "for file_id in file_ids:\n",
        "    raw_text = wordlists.raw(file_id)\n",
        "    raw_text = re.sub(pattern, '', raw_text)\n",
        "    words = word_tokenize(raw_text)\n",
        "\n",
        "unigrams = nltk.ngrams(words, 1)\n",
        "unigramFD = nltk.FreqDist(unigrams)\n",
        "\n",
        "bigrams = nltk.ngrams(words, 2)\n",
        "bigramFD = nltk.FreqDist(bigrams)\n",
        "\n",
        "trigrams = nltk.ngrams(words, 3)\n",
        "trigramFD = nltk.FreqDist(trigrams)\n",
        "\n",
        "# bigrams = nltk.ngrams(words, 2)\n",
        "# cfd1 = nltk.ConditionalFreqDist(bigrams)\n",
        "\n",
        "# print(cfd1)\n",
        "\n",
        "from nltk import ngrams\n",
        "\n",
        "n = 2  # Change this to 1 for unigrams and 3 for trigrams\n",
        "n_grams = list(ngrams(words, n))\n",
        "\n",
        "# Create a conditional frequency distribution for n-grams\n",
        "cfd = nltk.ConditionalFreqDist(n_grams)\n",
        "\n",
        "import random\n",
        "\n",
        "cfd1 = nltk.ConditionalFreqDist(bigrams)\n",
        "n=2\n",
        "\n",
        "length = random.randint(7,10)\n",
        "\n",
        "for i in range(4):\n",
        "\n",
        "    for j in range(4):\n",
        "\n",
        "        verse = [random.choice(words)]\n",
        "\n",
        "        for k in range(length-1):\n",
        "\n",
        "            context = verse[-1]\n",
        "            # print(context)\n",
        "            next_word = cfd[context].max()\n",
        "            verse.append(' ')\n",
        "            verse.append(next_word)\n",
        "\n",
        "        print(\" \".join(verse))\n",
        "\n",
        "    print(\" \")\n",
        "\n",
        "\n",
        "# Save all the generated verses to a new file\n",
        "# output_file = \"generated_verses.txt\"\n",
        "# with open(output_file, 'w', encoding='utf-8') as file:\n",
        "#     for verse in verses:\n",
        "#         file.write(verse + '\\n')\n",
        "\n",
        "# print(f'Generated verses saved to {output_file}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79601988",
      "metadata": {
        "id": "79601988"
      },
      "source": [
        "# Question2\n",
        " Classify language out of the list given below using just stop words. Remove punctuations, make it lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa286b84",
      "metadata": {
        "id": "fa286b84",
        "outputId": "eed1da3f-fabe-4ef1-ba32-dbf482163b85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'turkish']"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bdad25b",
      "metadata": {
        "id": "1bdad25b"
      },
      "outputs": [],
      "source": [
        "Test=\"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "266654b6",
      "metadata": {
        "id": "266654b6",
        "outputId": "38cd33e4-19c0-4338-a6af-6b2951888fe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'arabic': 0,\n",
              " 'azerbaijani': 1,\n",
              " 'basque': 0,\n",
              " 'bengali': 0,\n",
              " 'catalan': 3,\n",
              " 'chinese': 0,\n",
              " 'danish': 0,\n",
              " 'dutch': 3,\n",
              " 'english': 5,\n",
              " 'finnish': 0,\n",
              " 'french': 1,\n",
              " 'german': 1,\n",
              " 'greek': 0,\n",
              " 'hebrew': 0,\n",
              " 'hinglish': 8,\n",
              " 'hungarian': 1,\n",
              " 'indonesian': 1,\n",
              " 'italian': 2,\n",
              " 'kazakh': 0,\n",
              " 'nepali': 0,\n",
              " 'norwegian': 0,\n",
              " 'portuguese': 1,\n",
              " 'romanian': 1,\n",
              " 'russian': 0,\n",
              " 'slovene': 0,\n",
              " 'spanish': 1,\n",
              " 'swedish': 0,\n",
              " 'tajik': 0,\n",
              " 'turkish': 0}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your output looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a891b5-75b1-4a06-b756-fb69696e4f7c",
      "metadata": {
        "id": "38a891b5-75b1-4a06-b756-fb69696e4f7c",
        "outputId": "a8b008e0-abe1-4a48-f5d1-255e5bfc0736"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'arabic': 0,\n",
              " 'azerbaijani': 3,\n",
              " 'basque': 0,\n",
              " 'bengali': 0,\n",
              " 'catalan': 3,\n",
              " 'chinese': 0,\n",
              " 'danish': 0,\n",
              " 'dutch': 5,\n",
              " 'english': 9,\n",
              " 'finnish': 0,\n",
              " 'french': 1,\n",
              " 'german': 1,\n",
              " 'greek': 0,\n",
              " 'hebrew': 0,\n",
              " 'hinglish': 12,\n",
              " 'hungarian': 1,\n",
              " 'indonesian': 1,\n",
              " 'italian': 2,\n",
              " 'kazakh': 0,\n",
              " 'nepali': 0,\n",
              " 'norwegian': 0,\n",
              " 'portuguese': 1,\n",
              " 'romanian': 1,\n",
              " 'russian': 0,\n",
              " 'slovene': 0,\n",
              " 'spanish': 1,\n",
              " 'swedish': 0,\n",
              " 'tajik': 0,\n",
              " 'turkish': 0}"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "Test = re.sub(r'[^\\w\\s]', '', Test)\n",
        "Test = Test.lower()\n",
        "\n",
        "words = word_tokenize(Test)\n",
        "\n",
        "result_dict = {}\n",
        "\n",
        "for f_id in stopwords.fileids():\n",
        "\n",
        "    stop_words = set(stopwords.words(f_id))\n",
        "\n",
        "    count = 0\n",
        "    for w in words:\n",
        "        if w in stop_words:\n",
        "            count = count+1\n",
        "\n",
        "    result_dict[f_id] = count\n",
        "\n",
        "\n",
        "\n",
        "result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43708fa5",
      "metadata": {
        "id": "43708fa5"
      },
      "source": [
        "# Question 3\n",
        " Rule Based Roman Urdu Text Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e1f761e",
      "metadata": {
        "id": "2e1f761e"
      },
      "source": [
        "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
        "\n",
        "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a093fc6-fc29-4865-9f55-5e48fe57d174",
      "metadata": {
        "id": "2a093fc6-fc29-4865-9f55-5e48fe57d174"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "normrules_dict = {\n",
        "    \"ain$\": \"ein\",\n",
        "    \"ar\": \"r\",\n",
        "    \"ai\": \"ae\",\n",
        "    \"iy+\": \"i\",\n",
        "    \"ay$\": \"ae\",\n",
        "    \"ih+\": \"eh\",\n",
        "    \"s+\": \"s\",\n",
        "    \"ie$\": \"e\",\n",
        "    \"ry\": \"ri\",\n",
        "    \"es\": \"is\",\n",
        "    \"sy\": \"si\",\n",
        "    \"a+\": \"a\",\n",
        "    \"ty\": \"ti\",\n",
        "    \"j+\": \"j\",\n",
        "    \"o+\": \"o\",\n",
        "    \"e+\": \"i\",\n",
        "    \"([bcdefghijklmnopqrstuvwxyz])i\": r'\\1y',\n",
        "    \"d+\": \"d\",\n",
        "    \"u\": \"o\"\n",
        "}\n",
        "\n",
        "def normalize(word):\n",
        "    for pattern, replacement in normrules_dict.items():\n",
        "        word = re.sub(pattern, replacement, word)\n",
        "    return word\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "str = input('Enter String:')\n",
        "token_words = word_tokenize(str)\n",
        "\n",
        "n_words = []\n",
        "n_sent = []\n",
        "\n",
        "for word in token_words:\n",
        "    n_word = normalize(word)\n",
        "    n_words.append(n_word)\n",
        "\n",
        "    n_sent = ' '.join(n_words)\n",
        "\n",
        "print(n_sent)\n",
        "\n",
        "# zaroori zaruri zarori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc5d67f-88bc-4844-b3f8-ee3f77a9b517",
      "metadata": {
        "id": "2bc5d67f-88bc-4844-b3f8-ee3f77a9b517",
        "outputId": "f06f0226-628f-4c69-b711-810a80090350"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# import files\n",
        "\n",
        "#--------FOR LIST#1---------\n",
        "\n",
        "path1 = \"/home/muhammadzohaib/A1/Word dictionary/bigram_words.txt\"  # Replace with the path to your .txt file\n",
        "with open(path1, \"r\") as file:\n",
        "    word = file.readlines()\n",
        "\n",
        "list1 = []\n",
        "\n",
        "for line in word:\n",
        "    list1.append(line.strip())\n",
        "\n",
        "\n",
        "#--------FOR LIST#2---------\n",
        "\n",
        "path2 = \"/home/muhammadzohaib/A1/Word dictionary/words.txt\"\n",
        "\n",
        "with open(path2, \"r\") as file:\n",
        "    word = file.readlines()\n",
        "\n",
        "list2 = []\n",
        "\n",
        "for line in word:\n",
        "    list2.append(line.strip())\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "dictionary = list1 + list2\n",
        "\n",
        "# Replace 'your_text_file.txt' with the path to your text file\n",
        "with open(\"/home/muhammadzohaib/A1/Q1/word_test.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    unprocessed_text = file.read()\n",
        "\n",
        "\n",
        "# ...\n",
        "\n",
        "# Function to segment the sentence using n-grams for maximum matching with context\n",
        "def segment_with_context_ngrams(sentence, dictionary, n):\n",
        "    words = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        matched = False\n",
        "        for j in range(min(len(sentence), i + n, i), i, -1):\n",
        "            ngram = sentence[i:j]\n",
        "            if ngram in dictionary:\n",
        "                for k in range(j, min(len(sentence), j + n, j), j):\n",
        "                    next_ngram = sentence[j:k]\n",
        "                    if next_ngram in dictionary:\n",
        "                        words.append(next_ngram)\n",
        "                        i = k\n",
        "                        matched = True\n",
        "                        break\n",
        "                if not matched:\n",
        "                    words.append(ngram)\n",
        "                    i = j\n",
        "                    matched = True\n",
        "                    break\n",
        "        if not matched:\n",
        "            i += 1\n",
        "    return words\n",
        "\n",
        "# Segment the sentence using trigrams with context\n",
        "segmented_words_with_context_trigrams = segment_with_context_ngrams(unprocessed_text, dictionary, 2)\n",
        "segmented_sentence_with_context_trigrams = \" \".join(segmented_words_with_context_trigrams)\n",
        "print(segmented_sentence_with_context_trigrams)\n",
        "\n",
        "\n",
        "# print(\"Unprocessed Text:\", unprocessed_text)\n",
        "# print(\"Dictionary:\", dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9969e602-08e7-45ac-9659-429aae820b91",
      "metadata": {
        "id": "9969e602-08e7-45ac-9659-429aae820b91",
        "outputId": "8ef34a80-b17d-4c17-f247-7066ec25ba63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "تج ربہ کار ہند و س تان ی آ ف س پنر روی چند رنا یش ون نے آئن دہا یش یا ء کپ 2 0 2 3 ء کی غیر ی قی نی قسم تپ را پنی را ئے کا ا ظ ہار کیا ہے جوپ اکس تان میں ہون ے ج ارہ اہے اپن ے ی وٹ ی و بچی نل پرب ات کر تے ہوئ ے ر وی چند رنا یش ون نے کہا کہا گرپ ڑ و سیم لک بھا رتا یش یاک پ 2 0 2 3 ء میں شرک تکر ناچ اہت اہے توم قام تب دی لک ردی ناچ اہ یے \n",
            " آفس پنر نے کہا کہا نٹر نیش نل کرک ٹکو نسل ( آ ئی سیس ی ) ن ے پ اکس تان کوٹ ورن امن ٹکی میز بان یک احق دے دیا ہے لیک نبھ ارت پاک ستا نکا دور ہک رن ے کوت یار نہی ں ر وی چند رنا یش ون نے بھی 2 0 2 3 ء میں 5 0 او ورک ے و رل ڈک پکے حوا لے سے پاک ستا نکر کٹ بور ڈ ( پ ی س ی ب ی ) کے حال یہ بیا نکا جوا بدی تے ہوئ ے کہا میر ے خ یال میں یہ مم کن نہی ں ہے آفس پنر نے م زید کہا کہ پاک ستا نن ے پہل ے ب ھا رت کا دور ہک رن ے سے انک ارک ردی ات ھا لیک ن آ خر کار وہم یگ ا ایو نٹ سمی ں شرک تکے لیے بھا رت گئے تھے \n",
            " غ ور طلب ہے کہا یش یاک پکے معا ملے پر حال ہی میں بحر ین میں ہون ے و الی ایک ہن گام یم ی ٹ نگم ی ں غور کیا گیا جہا ں و ینی و ک ے ب ارے میں حتم یف ی صلہ مار چت کم و خر کرد یا گیا بحر ین میں ایش ین کرک ٹکو نسل ( ا ے س ی س ی ) کے اجل اس کے بعد بیس ی س ی آ ئی کے حک امن ے ا علا نکی اکہ بور ڈن ے ایو نٹ کے لیے اپن ی ٹ یم پاک ستا نن ہ بھی جنے کاف ی ص لہک یا ہے تاہ م پ ی س ی ب یکے حک امن ے بھی سخت رد عمل کا ا ظ ہار کر تے ہوئ ے ک ہا ہے کہو ہاک توب رمی ں ب ھا رتم ی ں ہون ے وال ے و رل ڈک پ 2 0 2 3 ء میں شرک تنہ ی ں کری ں گ ے \n",
            " توش ہ خ انہ کیس میں عمر ان خان پر فرد جرم عا ئ دن ہ ہوس کیس یش ن عد ال تنے الی کشن کمی ش ن کیچ ی ئ رمی نپ ی ٹی آئی کی خلا فف وجد اری م ق دمے کید رخ وا ست پر عمر ان خان کی طبی بنی اد وں پر آ ج حا ضر ی سے است ث ن ٰ یک ید رخ وا ستم ن ظ ورک رلی الی کشن کمی ش ن کی جان بسے پیٹ ی آ ئی و کلا کوت صدی ق ش دہک اپی ا ں فرا ہمن ہیں کی گئی ں ت ص دی ق شدہ کا پیا ں کیف راہ میک ے ب عد فرد جرم کیا گلی تار یخ مقر رکی جا ئے گی \n",
            " ت فصی لات کے مطا بق سی شنع دال تمی ں ا لیک ش ن کمی ش ن کی عمر ان خان کے خلا فف وجد اری م ق دمے کید رخ وا ست پرس ماع تہ و ئی جج ظفر ا قبا لن ے در خوا ست پرس ماع تکی عمر ان خان کی جان بسے طبی بنی اد وں پر حا ضر ی سے است ثنی ٰ ک ید رخ وا ستد ائر کی گئی عد ال تنے وکی لسے است ف س ارک یاک ہک یام چلک ے ج مع کرو ادی ئے وکی لگو ہر علی خان نے کہا کہ عمر ان خان کے ضم انت یم چلک ے ج مع کرو ادی ئے جج نے است ف س ارک یاک ہا ی سے حا ضر ی سے است ث ن ٰ یک ید رخ وا ستد ائر ہوت ی ر ہی تو فرد جرم کیس ے ع ا ئ دہو گی وکی لعل ی ب خار ین ے بتا یاک ہ ہمی ں مصد ق ہکا پیا ں ف راہ منہ ی ں کی گئی ں \n",
            " جج نے ہد ایت کیک ہت مام ث بو توں کی ت ص دی ق شدہ کا پیا ں عد ال تا ور پیٹ ی آئی کو فرا ہمک ری ں وکی لال یک ش نک میش نن ے کہا کہہ م آ جہ یکم پلی نٹا ور ث بو توں کیم صدق ہکا پیا ں ف راہ مکر دیں گے وکی لال یک ش نک میش نن ے کہا کہ عمر ان خان اب ھی تک عد ال تکی وں نہی ں آئے جج نے است ف س ارک یاک ہک یاک ہم جھ ے ایک تار یخ بتا دیں عمر ان خان کب عد ال ت آئی ں گ ے ع مرا نخ انک ے وکی لن ے کہا کہ عمر ان خان کی صحت نے اج از تد یت و آئی ں گ ے ڈ اکٹ رز کیہ دا یت پر عمل کر رہے ہیں عد ال تنے عمر ان خان کی آ ج حا ضر ی سے است ثنی ٰ پ رف ی صلہ م حف و ظ ک رلی ا ع دال تنے طبی بنی اد وں پر عمر ان خان کی آ ج حا ضر ی سے است ثنی ٰ ک ید رخ وا ستم ن ظ ورک رلی چی ئ رمی نپ ی ٹی آئی عمر ان خان پر فرد جرم عا ئ دن ہ ہوس کی \n",
            " یاد رہے کہ توش ہ خان ہک ی سمی ں ع مرا نخ انن ے سین ئ ر وکی لخ وا جہ حار ث ک یخ دم ات حا صل کر لیں خوا جہ حار ث سمی ت 4 و کلا کے و کال تنا مے عد ال تمی ں ج مع کرا دی ئے گئے ہیں \n",
            " س اب قو فاق یوز ی ر مفت ا حا سما عیل کاک ہنا ہے کہو زیر خز انہ اس حاق ڈار نے پھر وہی حرک تکی جوش و ک تت رین نے کی تھی انہ وں نے آئی ایم ایف سے کیا گیا معا ہدہ توڑ ات ھا انہ وں نے ہمن یوز کے پرو گرا ممی ں گ ف ت گو کر تے ہوئ ے م زید کہا کہ آئی ایم ایف سے معا ہدہ ہون ے ک ے ب عد چیز ی ں ب ہت رہو ں گیپ ی آ ئی اے اس سال 9 0 ارب روپ ے ک ان قصا نکر ے گ یل یک ناگ رپ ی آئی اےک ین جک اری ہو گیت و 9 0 ارب روپ ے ک ان قصا نن ہیں ہو گا \n",
            " مفت ا حا سما عیل نے کہا کہ میں تو چاہ رہا تھا پہل ے د نہی پٹ رول یم مص نوع ات کی قیم توں میں اضا فہ کیا جا ئے سیل اب آنے سے پہل ے ہمن ے ڈ یف الٹ رسک کوک مکی ات ھا شہب از شری فا گر آ جوز ی ر ا ع ظ م نہہ و ت ے ت و م لک ڈی فال ٹکی طرف جا چکا تھا قبل از ی ں ایک بیا نمی ں م فتا حا سما عیل نے کہا کہہ مسل ملی گم ی ں کوئ یف ار ورڈ بلا کن ہیں بن رہا پاک ستا ن ڈ یف الٹ نہی ں ک رے گا پار ٹیک ے م و ج ود ہنا ئ بصد رک و پار ٹیک ے ف ی ص لے کرن ے کا اخت یار حا صلہ ے وہ گز شتہ روز کرا چی میں نیش نلا سلا مکا کنا مک کان فرن سسے خطا بکر رہے تھے \n",
            " س اب قوز ی رخ زان ہن ے ا پنے خطا با ورم ی ڈی اسے بات چیت کے دور ان مسل ملی گم ی ں فار ورڈ بلا کب نن ے کی خبر وں کوم ستر دک رت ے ہوئ ے ک ہاک ہا ی سیا طلا عا تمی ں کوئ ی ص دا قت نہی ں ہے شاہ دخا قان عبا سیپ ہلے ہیک ہ ہچک ے ہیں کہو ہپا رٹی میں ہی ہیں انہ وں نے اپن ے پ ار ٹی عہد ہ س ے ا ست عفی ٰ دیا ہے پار ٹین ہیں چھو ڑ یہ ے ملک کیم عا شی صور ت حال کے حوا لے سے مفت ا حا سما عیل کاک ہنا تھا کہ ملک کے ڈی فال ٹ ہون ے ک اک و ئی ام کان ہیں ا ٓ ئی ایم ایف سے ڈیل ہو جا ئی گیا ورم عام لات درس تہ و جا ئی نگے انک اکہ نا تھا کہا گر شوک تت رین فرو ریم ی ں ا ٓ ئ یا یم ایف کام عا ہدہ نہ توڑ تے تو حال اتا چھے ہوت ے پ ی ٹ یا ٓ ئی کی حکو مت میں 8 0 ارب ڈال رکی ام پور ٹ ہوئ ی ڈ ال رک ور و ک کر رکھ نا غلط یت ھی اور اسل ئے ڈال رکی قیم تا ٓ جبل ند تری نس ط ح پرہ ے ڈ ال رکی قدر کو پکڑ کر روک انہ ی ں جا سکت اڈا ل ر کاف ری فلو ٹرہ ناہ ی ب ہت رہے غیر ضرو ریم شین ری کوہ منے ام پور ٹس ے ر و ک ات ھا اب ھی جوپ ابن دی ہے وہ کچھ اور ہے \n",
            " وزی ردا خلہ ران ا ث نا ء ال لہک اکہ ناہ ے ک ہ ع مرا نخ انن ے جیل بھر و ت حر یک کا ا ع لان کیا ہے وہپ ہلے بھی اس ہتھ کنڈ ے م ی ں ناک امہ و ئ ے ع مرا نخ انک و مع لو مہ ین ہیں کہ جیل میں رہن اکت نام شکل ہے میڈ یار پور ٹسک ے م طاب قرا نا ثنا ء ال لہ نے اپن ے ب یان میں کہا کہ عمر ان خان کام قصد سیا سیا فرا تف ریہ ے وہا سمی ں ن اکا مہ وں گے عمر ان خان اپن ی ز ند گی کا صرف ایک دن جیل میں رہے \n",
            " ا ے پ ی س یم ی ں تما مج ماع توں کیش رکت کے لیے 2 دنب ڑ ھا دی ئے ہیں اے پیس یم ی ں کسی جما عت کے شام لن ہ ہون ے س ے م عام لہر کن ہیں سکت اوز ی ر داخ لہک اکہ نا تھا کہ قوم یم عام لے کات مام سیا سیج ماع توں کوم لک رحل نکا لن اہو گا اے پیس یم ی ں ات فاق را ئے کے بعد حکو مت دہش تگر دوں کے خلا فک ار روا ئی کاف ی ص لہک رے گی \n",
            " قبل از ی ں وفا قی وزی را یا ز صاد قن ے کہا کہ پرو ی ز خ ٹکا سد قیص راو را ع جاز شاہ کوا ے پ ی س یم ی ں شرک تکی د ع وتد یان سے کہا کہو ہی ہد ع و ت عمر ان خان تک پہن چا دیں \n",
            " ایا ز ص ادق نے پیٹ ی آئی رہن ما اسد قیص رکے بیا نپ ر رد ِ ع مل دیت ے ہوئ ے ک ہا ہے کہا ے پ ی س یم ی ں شرک تکی لئے کسی کوب ھی د ع و ت نام ہن ہیں بھج وا یا فون کیے یام لا قا تکر کے کان فرن سمی ں ش رکت کید ع و تد یگ ئی ہے سا بقا سپی کر قوم یاس مبل یاس دقی صر روی وں کی بات نہک ری ں توا چھا ہے پیٹ ی آئی نے نلی گپی پل زپ ار ٹیک ے س ات ھا یف آئی اے اور نیب کے ذری ع ے ک یاک چھن ہیں کیا جب کہ تحر یک ان صاف کے رہن ما اسد قیص رکا کہن ات ھا کہ مجھ ے حکو متی نما ئ ن دوں نے فون پرا ے پ ی س یم ی ں شرک تکی د ع وتد یک ان فرن سکی د ع وتد ین ے کا یہ بال کلب ھی منا سبط ریق ہن ہیں ہے اے پیس یم ی ں د ع و تد ین اکا ا حسن طری ق ہ ہوت اہے ہمس مجھ تے ہیں کہ ملک سنگ ین بحر انو ں ک اشک ارہ ے ا ورا سوق تیک جہت یک ی ضرو رت ہے حکو متی روی ہ غ ی ر آئی نی ہے اور اسم ا حول میں یک جہت یک اس وال ہی پید انہ ی ں ہوت ا حکو مت کوس بسے پہل ے ا پنے روی ے م ی ں بہت ریل انا ہو گی ہمن ے ا پنے دور میں دہش تگر دیپ رقا بو پان ے کیل ئے بہت رین حکم ت ع ملی بنا کرا من قا ئ مکی ا\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.util import ngrams\n",
        "# Replace 'your_text_file.txt' with the path to your text file\n",
        "with open(\"/home/muhammadzohaib/A1/Q1/word_test.txt\", \"r\") as file:\n",
        "    unprocessed_text = file.read()\n",
        "\n",
        "\n",
        "# Function to find the best word based on the dictionary\n",
        "def find_best_word(ngram, dictionary):\n",
        "    for i in range(len(ngram), 0, -1):\n",
        "        if ngram[:i] in dictionary:\n",
        "            return ngram[:i]\n",
        "    return ngram[0]\n",
        "\n",
        "# Tokenize the unprocessed text using n-grams and the dictionary\n",
        "def tokenize_with_ngrams(text, dictionary, n):\n",
        "    word_list = []\n",
        "    words = set(dictionary)\n",
        "\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        for j in range(i + n, i, -1):\n",
        "            ngram = text[i:j]\n",
        "            if ngram not in words:\n",
        "                ngram = text[i:i + n]\n",
        "                segmented_word = find_best_word(ngram, dictionary)\n",
        "                word_list.append(segmented_word)\n",
        "                i += len(segmented_word)\n",
        "            else:\n",
        "                word_list.append(ngram)\n",
        "                i = j\n",
        "                break\n",
        "\n",
        "\n",
        "    return word_list\n",
        "\n",
        "# Tokenize the unprocessed text using n-grams\n",
        "n = 3  # You can adjust 'n' based on your needs\n",
        "tokenized_text = tokenize_with_ngrams(unprocessed_text, dictionary, n)\n",
        "\n",
        "# Display the segmented text\n",
        "segmented_text = \" \".join(tokenized_text)\n",
        "print(segmented_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e477ee4-a4ac-4ae8-ba8c-50882215c948",
      "metadata": {
        "id": "8e477ee4-a4ac-4ae8-ba8c-50882215c948",
        "outputId": "22de5ec5-b8b0-43a1-e460-eb87ba6fbe56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# import files\n",
        "\n",
        "#--------FOR LIST#1---------\n",
        "\n",
        "path1 = \"/home/muhammadzohaib/A1/Word dictionary/bigram_words.txt\"  # Replace with the path to your .txt file\n",
        "with open(path1, \"r\") as file:\n",
        "    word = file.readlines()\n",
        "\n",
        "list1 = []\n",
        "\n",
        "for line in word:\n",
        "    list1.append(line.strip())\n",
        "\n",
        "\n",
        "#--------FOR LIST#2---------\n",
        "\n",
        "path2 = \"/home/muhammadzohaib/A1/Word dictionary/words.txt\"\n",
        "\n",
        "with open(path2, \"r\") as file:\n",
        "    word = file.readlines()\n",
        "\n",
        "list2 = []\n",
        "\n",
        "for line in word:\n",
        "    list2.append(line.strip())\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "dictionary = list1 + list2\n",
        "\n",
        "# Replace 'your_text_file.txt' with the path to your text file\n",
        "with open(\"/home/muhammadzohaib/A1/Q1/word_test.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    unprocessed_text = file.read()\n",
        "\n",
        "\n",
        "# ...\n",
        "\n",
        "# Function to segment the sentence using n-grams for maximum matching with context\n",
        "def segment_with_context_ngrams(sentence, dictionary, n):\n",
        "    words = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        matched = False\n",
        "        for j in range(min(len(sentence), i + n, i), i, -1):\n",
        "            ngram = sentence[i:j]\n",
        "            if ngram in dictionary:\n",
        "                for k in range(j, min(len(sentence), j + n, j), j):\n",
        "                    next_ngram = sentence[j:k]\n",
        "                    if next_ngram in dictionary:\n",
        "                        words.append(next_ngram)\n",
        "                        i = k\n",
        "                        matched = True\n",
        "                        break\n",
        "                if not matched:\n",
        "                    words.append(ngram)\n",
        "                    i = j\n",
        "                    matched = True\n",
        "                    break\n",
        "        if not matched:\n",
        "            i += 1\n",
        "    return words\n",
        "\n",
        "# Segment the sentence using trigrams with context\n",
        "segmented_words_with_context_trigrams = segment_with_context_ngrams(unprocessed_text, dictionary, 3)\n",
        "segmented_sentence_with_context_trigrams = \" \".join(segmented_words_with_context_trigrams)\n",
        "print(segmented_sentence_with_context_trigrams)\n",
        "\n",
        "\n",
        "# print(\"Unprocessed Text:\", unprocessed_text)\n",
        "# print(\"Dictionary:\", dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8239c08-6cb7-4a57-9a12-6c607d479a56",
      "metadata": {
        "id": "c8239c08-6cb7-4a57-9a12-6c607d479a56"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f88b2d2-93fb-42e0-a412-18912b3a97ef",
      "metadata": {
        "id": "0f88b2d2-93fb-42e0-a412-18912b3a97ef"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}